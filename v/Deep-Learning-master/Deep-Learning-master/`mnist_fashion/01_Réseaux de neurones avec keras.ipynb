{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/3_logos.png\" width = 600, align = \"center\">\n",
    "<br>\n",
    "<h1 align=center>Réseaux de neurones avec Keras</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 – TensorFlow Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow Playground](http://playground.tensorflow.org).\n",
    "\n",
    "- **Layers et patterns**: essayez d’entraîner le réseau de neurones par défaut en cliquant sur le bouton \"Exécuter\" (en haut à gauche). Remarquez comment il trouve rapidement une bonne solution pour la tâche de classification. Notez que les neurones de la première couche cachée ont appris des motifs simples, tandis que les neurones de la deuxième couche cachée ont appris à combiner les motifs simples de la première couche cachée en des motifs plus complexes. En général, plus il y a de couches, plus les motifs peuvent être complexes.\n",
    "\n",
    "- **Fonction d'activation**: essayez de remplacer la fonction d'activation Tanh par la fonction d'activation ReLU, puis entraînez à nouveau le réseau. Notez qu'il trouve une solution encore plus rapidement, mais cette fois, les limites sont linéaires. Cela est dû à la forme de la fonction ReLU.\n",
    "\n",
    "- **Minima locaux**: modifiez l’architecture du réseau pour n’avoir qu’une couche cachée avec trois neurones. Entraînez-le plusieurs fois (pour réinitialiser les poids du réseau, ajoutez et supprimez un neurone). Notez que le temps d’entraînement varie beaucoup et que parfois, il s’arrête sur un minimum local.\n",
    "\n",
    "- **Trop petit**: supprimez maintenant un neurone pour ne garder que 2. Notez que le réseau de neurones est maintenant incapable de trouver une bonne solution, même si vous essayez plusieurs fois. Le modèle a trop peu de paramètres et sous-complète systématiquement l'ensemble de formation.\n",
    "\n",
    "- **Assez grand**: ensuite, définissez un nombre de neurones à 8 et entraînez le réseau plusieurs fois. Notez qu'il est maintenant constamment rapide et ne reste jamais bloqué. Cela met en évidence une découverte importante de la théorie des réseaux de neurones: les grands réseaux de neurones ne se coincent presque jamais dans les minima locaux, et même lorsqu'ils le font, ces optima locaux sont presque aussi bons que l'optimum global. Cependant, ils peuvent rester bloqués sur de longs plateaux pendant longtemps.\n",
    "\n",
    "- **Gradients profonds et nuls**: changez maintenant le jeu de données en spirale (jeu de données en bas à droite sous \"DONNEES\"). Changez l'architecture du réseau pour avoir 4 couches cachées de 8 neurones chacune. Notez que l'entraînement prend beaucoup plus de temps et reste souvent bloqué sur des plateaux pendant de longues périodes. Notez également que les neurones situés dans les couches les plus hautes (c’est-à-dire à droite) ont tendance à évoluer plus rapidement que les neurones situés dans les couches les plus basses (c.-à-d. À gauche). Ce problème, appelé le problème des \"gradients disparus\", peut être résolu en utilisant une meilleure initialisation du poids et d'autres techniques, de meilleurs optimiseurs (tels que AdaGrad ou Adam) ou en utilisant la normalisation par lots.\n",
    "\n",
    "- **Ensuite**: jouez pendant au moins une heure quand vous avez le temps !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 – Classification d'images Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger le jeu de données fashion MNIST. Keras a un certain nombre de fonctions (dont load_data()) pour charger des jeux de données populaires dans `keras.datasets`. Le jeu de données est déjà divisé en un jeu d'apprentissage et un jeu de tests, mais il peut être utile de fractionner davantage le jeu d'apprentissage pour obtenir un jeu de validation(5000 echantillons):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher la taille du jeu d'entrainement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher la première ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher l'echantilon en utilisant la fonction `imshow()` avec un color map en `'binary'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher les différentes valeurs des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noms des classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première image correspond donc à un coat :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher les 50 premières images (utiliser des subplots en adaptant la taille de la figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction d'un réseau de neurones de classification avec Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1)\n",
    "Construire un modèle séquentiel (keras.models.Sequential),(voir [doc](https://keras.io/models/about-keras-models/) ), sans aucun argument, puis ajoutez-y quatre couches en utilisant la méthode add ():\n",
    " - une couche Flatten (keras.layers.Flatten) pour convertir chaque image 28x28 en une rangée unique de 784 pixels. Comme il s'agit de la première couche du modèle, il faut spécifier l'argument input_shape en laissant de côté la taille du lot: [28, 28].\n",
    "- une couche dense (keras.layers.Dense) avec 300 neurones (unités aka) et la fonction d'activation \"relu\".\n",
    "- Une autre couche dense avec 100 neurones, également avec la fonction d'activation \"relu\".\n",
    "- Une couche dense finale avec 10 neurones (un par classe) et la fonction d'activation \"softmax\" garantissant que la somme de toutes les probabilités de classe estimées pour chaque image est égale à 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2)\n",
    "Utiliser la méthode `summary()` pour étudier l'output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Compilation\n",
    "Une fois le modèle créé, il faut utiliser la méthode `compile ()` pour spécifier la fonction `loss` et l'` optimiseur` à utiliser. On utilisera \"sparse_categorical_crossentropy\" pour le loss et l'optimiseur \"sgd\" (descente de gradient stochastique), spécifier également `metrics` à `accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4)\n",
    "Entrainer le modèle en utilisant la méthode `fit()`<br/>\n",
    "**Remarque**: la méthode `fit ()`retourne un objet `History`contenant des statistiques d'entraînement. Il peut être utile de conserver le résultat dans une variable : (history = model.fit (...))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5)\n",
    "Construire un dataframe avec l'objet history puis en faire une représenatation graphique (méthode plot())<br>\n",
    "*Conseils* : \n",
    "- Afficher la grille et limiter les ordonnées à l'intervalle \\[0, 1\\]\n",
    "- Il peut être utile de créer une fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6)\n",
    "Relancer `model.fit()` , et remarquer qu'il reprend là où il s'était arrété."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7)\n",
    "Evaluer le modèle avec la méthode `evaluate()`(as-t-on précision autour de 80% ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8) Prédiction\n",
    "Définir X_new par les 10 premières instances du jeu de tests. Utiliser la méthode de prédict() du modèle pour estimer la probabilité de chaque classe pour chaque instance (arrondir les résultats au 100ème):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9)\n",
    "Utilisez `np.argmax ()`pour obtenir l'ID de classe de la classe la plus probable pour chaque instance. *Utiliser 'axis=1'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10) méthode predict_classes\n",
    "\n",
    "Utiliser la méthode `predict_classes()` pour `X_new`. On devrait obtenir les mêmes résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11)\n",
    "Utiliser `np.max()`pour déterminer une estimation de la probabilité associée à chaque classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 – Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1)\n",
    "Pour l'utilisation de la descente de gradient, il est généralement préférable de s'assurer que les entités soient mises à l'échelle, de préférence avec une distribution normale. Uniformiser les valeurs des pixels et tester les performances du réseau.\n",
    "\n",
    "- *Utiliser `StandardScaler`de Scikit-Learn.\n",
    "- *Relancer la méthode fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2)\n",
    "Tracer les courbes d'apprentissage et comparer avec celles obtenues dans la première partie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4 – Les Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1)\n",
    "La méthode `fit()` accepte un argument `callbacks`. Entrainer le modèle avec un grand nombre de `epoch` en y ajoutant les *callbacks* suvants (les callback proviennent de la librairie `keras.callbacks` et sont définis dans un tableau :\n",
    "* `EarlyStopping`: avec `patience=5` pour arrêter l'apprentissage dès que le modèle ne change plus (5 epoch de marge)\n",
    "* `ModelCheckpoint`: specifie le nom du modèle à enregistrer (exemple `\"mnist_fashion.h5\"`) avec `save_best_only=True` pour enregistrer le meilleur modèle obtenu.\n",
    "\n",
    "callbacks=\\[keras.callbacks.EarlyStopping......,\n",
    "    keras.callbacks........\\]<br/>\n",
    "history = model.fit(............,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "Le callback `EarlyStopping` permettra de réduire les risques de sur-apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2)\n",
    "\n",
    "Grâce aux callbacks, le dernier modèle enregistré est le meilleur sur le jeu de validation.<br>\n",
    "Charger le modèle avec 'keras. Models. load_model () et l'évaluer sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une liste de callbacks est disponible sur https://keras.io/callbacks/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
